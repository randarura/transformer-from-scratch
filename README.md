# Transformer from Scratch

This repository contains a minimal, from-scratch implementation of the **Transformer** architecture, following the original ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) paper.

## Features
- Scaled Dot-Product Attention
- Multi-Head Attention with residual connections & LayerNorm
- Position-wise Feed-Forward layers
- Sinusoidal Positional Encoding
- Stackable Encoder layers
- Example training loop with dummy data
- Easy to extend for Decoder and full Seq2Seq tasks

## File Structure
